{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "The solution is adapted from the implementation of a DECI model in the causica library, which can be found [here](https://github.com/microsoft/causica/blob/b3c79a01f30f44ed36c582ffe2b4522058d82a73/causica/models/deci/deci.py), with an associated paper of the original model [here](https://arxiv.org/abs/2202.02195).\n",
    "\n",
    "The solution is GNN-based, DECI being a generative model that employs an additive noise structural equation model to capture the functional relationships among variables and exogenous noise, while also learning a variational distribution. It is designed to perform causal inference without background information about the causal graph.\n",
    "\n",
    "The relationships are learnt through flexible neural networks, while the noise can be modeled either as a Gaussian or a spine-flow model. It is considered a generative method, since it essentially evolves from exogenous noise to observations.\n",
    "\n",
    "Both a mean-field approximate posterior distribution, and the functional relationships, are learned by optimising an evidence lower bound (ELBO).\n",
    "\n",
    "The implementation itself is based on the popular torch library, and on the causica library, following these steps: it creates prior distributions over directed acyclic graphs (DAGs), it also creates variational posterior distributions over the adjacency matrices, a GNN, for representing functional relationships, and a noise distribution for each node, these last three components being the ones to be optimized. These components are gathered in a structural equation model (SEM), which will be optimized with different learning rates for each module. An Augmented Lagrangian scheduler is employed, that will optimize towards a DAG. The result of the training process will be the adjacency matrix.\n",
    "\n",
    "Since it is already available, we also try to employ the discretization method, used in the bnlearn solution, to verify whether it helps in any way the learning process."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3daf339e4a959bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Preliminaries\n",
    "\n",
    "This section includes the required imports, and it also defines the hyperparameters. The notebook can be run locally after following the README instructions in the root directory."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44cc2328921b7ddb"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from causica.distributions import ContinuousNoiseDist\n",
    "\n",
    "from data.csuite.csuite_datasets import *\n",
    "from data.sachs.sachs_datasets import unaltered_dataset\n",
    "from evaluation.metrics import eval_all\n",
    "from models.deci.causica_deci import causica_deci\n",
    "from utils.solution_utils import hartemink_discretization"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T22:41:49.912621200Z",
     "start_time": "2024-05-11T22:41:43.731617500Z"
    }
   },
   "id": "690a8dd96c9997a4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_config = {\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 1000,\n",
    "    \"init_alpha\": 0.0,\n",
    "    \"init_rho\": 1.0,\n",
    "    \"gumbel_temp\": 0.25,\n",
    "    \"prior_sparsity_lambda\": 5.0,\n",
    "    \"embedding_size\": 32,\n",
    "    \"out_dim_g\": 32,\n",
    "    \"num_layers_g\": 2,\n",
    "    \"num_layers_zeta\": 2,\n",
    "    \"noise_dist\": ContinuousNoiseDist.SPLINE\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T22:41:49.918915700Z",
     "start_time": "2024-05-11T22:41:49.915703600Z"
    }
   },
   "id": "37321f09b0760d75"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Observational Set (Sachs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95eb39886de4b61b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~ WITHOUT DISCRETIZATION ~~~~~\n",
      "epoch:0 loss:3.0279e+05 nll:3.0279e+05 dagness:5.34188 num_edges:28 alpha:0 rho:1 step:0|1 num_lr_updates:0\n",
      "epoch:100 loss:11934 nll:11934 dagness:38.23705 num_edges:53 alpha:0 rho:1 step:0|701 num_lr_updates:0\n",
      "epoch:200 loss:2990.1 nll:2989.8 dagness:28.87832 num_edges:50 alpha:0 rho:1 step:0|1401 num_lr_updates:0\n",
      "epoch:300 loss:1755 nll:1754.7 dagness:22.02489 num_edges:48 alpha:0 rho:1 step:0|2101 num_lr_updates:0\n",
      "epoch:400 loss:1189.5 nll:1189.2 dagness:30.51480 num_edges:48 alpha:0 rho:1 step:0|2801 num_lr_updates:0\n",
      "Updating alpha to: 25.203094482421875\n",
      "epoch:500 loss:497.13 nll:496.89 dagness:1.17651 num_edges:36 alpha:25.203 rho:1 step:1|501 num_lr_updates:0\n",
      "epoch:600 loss:492.5 nll:492.27 dagness:1.17651 num_edges:34 alpha:25.203 rho:1 step:1|1201 num_lr_updates:0\n",
      "epoch:700 loss:258.21 nll:257.99 dagness:1.17651 num_edges:33 alpha:25.203 rho:1 step:1|1901 num_lr_updates:0\n",
      "epoch:800 loss:197.57 nll:197.35 dagness:1.17651 num_edges:32 alpha:25.203 rho:1 step:1|2601 num_lr_updates:0\n",
      "Updating alpha to: 26.379602432250977\n",
      "epoch:900 loss:333.58 nll:333.39 dagness:1.17651 num_edges:29 alpha:26.38 rho:1 step:2|301 num_lr_updates:0\n",
      "{'adjacency_f1': 0.41860464215278625, 'orientation_f1': 0.23255813121795654}\n",
      "\n",
      "\n",
      "\n",
      "~~~~~ WITH DISCRETIZATION ~~~~~\n",
      "Working on the 60th level of discretization.\n",
      "Working on the 50th level of discretization.\n",
      "Working on the 40th level of discretization.\n",
      "Working on the 30th level of discretization.\n",
      "Working on the 20th level of discretization.\n",
      "Working on the 10th level of discretization.\n",
      "epoch:0 loss:20.124 nll:20.026 dagness:12.02147 num_edges:29 alpha:0 rho:1 step:0|1 num_lr_updates:0\n",
      "epoch:100 loss:-9.1008 nll:-9.1802 dagness:0.50418 num_edges:18 alpha:0 rho:1 step:0|701 num_lr_updates:0\n",
      "epoch:200 loss:-17.058 nll:-17.073 dagness:0.16677 num_edges:7 alpha:0 rho:1 step:0|1401 num_lr_updates:1\n",
      "epoch:300 loss:-14.19 nll:-14.228 dagness:0.00000 num_edges:11 alpha:0 rho:1 step:0|2101 num_lr_updates:1\n",
      "epoch:400 loss:-21.45 nll:-21.524 dagness:1.27047 num_edges:17 alpha:0 rho:1 step:0|2801 num_lr_updates:1\n",
      "Updating alpha to: 2.172323226928711\n",
      "epoch:500 loss:-2.3217 nll:-2.4139 dagness:1.23676 num_edges:17 alpha:2.1723 rho:1 step:1|501 num_lr_updates:1\n",
      "Updating alpha to: 3.2821884155273438\n",
      "epoch:600 loss:-0.98977 nll:-1.0773 dagness:0.00000 num_edges:16 alpha:3.2822 rho:1 step:2|605 num_lr_updates:0\n",
      "Updating alpha to: 16.41094207763672\n",
      "epoch:700 loss:-9.6024 nll:-9.7272 dagness:0.67233 num_edges:20 alpha:16.411 rho:1 step:3|436 num_lr_updates:0\n",
      "epoch:800 loss:-15.292 nll:-15.38 dagness:0.00000 num_edges:16 alpha:16.411 rho:1 step:3|1136 num_lr_updates:1\n",
      "epoch:900 loss:-19.208 nll:-19.308 dagness:0.00000 num_edges:18 alpha:16.411 rho:1 step:3|1836 num_lr_updates:2\n",
      "Updating alpha to: 82.0547103881836\n",
      "{'adjacency_f1': 0.3030303120613098, 'orientation_f1': 0.06060606241226196}\n"
     ]
    }
   ],
   "source": [
    "# Without discretization\n",
    "print(\"~~~~~ WITHOUT DISCRETIZATION ~~~~~\")\n",
    "df, gt_graph = unaltered_dataset(get_data=True, return_index_name_correlation=False, return_adjacency_graph=True)\n",
    "pred_graph = causica_deci(df, train_config)\n",
    "print(eval_all(torch.tensor(pred_graph), gt_graph))\n",
    "\n",
    "# With discretization\n",
    "print(\"\\n\\n\\n~~~~~ WITH DISCRETIZATION ~~~~~\")\n",
    "df = hartemink_discretization(df)\n",
    "pred_graph = causica_deci(df, train_config)\n",
    "print(eval_all(torch.tensor(pred_graph), gt_graph))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T23:16:44.930080300Z",
     "start_time": "2024-05-11T22:41:49.918915700Z"
    }
   },
   "id": "3cfd4cbcb647dcf8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Synthetic Set (CSuite)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "148b833ef3e269a0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~ WITHOUT DISCRETIZATION ~~~~~\n",
      "epoch:0 loss:3.0199 nll:3.0179 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:0|1 num_lr_updates:0\n",
      "Updating alpha to: 0.0\n",
      "epoch:100 loss:2.771 nll:2.7686 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:1|335 num_lr_updates:0\n",
      "Updating alpha to: 0.0\n",
      "epoch:200 loss:2.486 nll:2.4835 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:2|611 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "Updating alpha to: 0.0\n",
      "epoch:300 loss:2.5766 nll:2.5741 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:4|131 num_lr_updates:0\n",
      "Updating alpha to: 0.0\n",
      "epoch:400 loss:2.7435 nll:2.741 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|438 num_lr_updates:0\n",
      "epoch:500 loss:2.6145 nll:2.612 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|2038 num_lr_updates:1\n",
      "epoch:600 loss:2.6867 nll:2.6842 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|3638 num_lr_updates:1\n",
      "epoch:700 loss:2.727 nll:2.7246 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|5238 num_lr_updates:1\n",
      "epoch:800 loss:2.7463 nll:2.7438 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|6838 num_lr_updates:1\n",
      "epoch:900 loss:2.6649 nll:2.6624 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|8438 num_lr_updates:1\n",
      "{'adjacency_f1': 1.0, 'orientation_f1': 0.0}\n",
      "\n",
      "\n",
      "\n",
      "~~~~~ WITH DISCRETIZATION ~~~~~\n",
      "Working on the 60th level of discretization.\n",
      "Working on the 50th level of discretization.\n",
      "Working on the 40th level of discretization.\n",
      "Working on the 30th level of discretization.\n",
      "Working on the 20th level of discretization.\n",
      "Working on the 10th level of discretization.\n",
      "epoch:0 loss:3.1887 nll:3.1868 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:0|1 num_lr_updates:0\n",
      "Updating alpha to: 0.0\n",
      "epoch:100 loss:-0.5363 nll:-0.53618 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:1|441 num_lr_updates:0\n",
      "Updating alpha to: 0.0\n",
      "epoch:200 loss:-2.3873 nll:-2.387 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:2|1181 num_lr_updates:0\n",
      "epoch:300 loss:-3.2756 nll:-3.2753 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:2|2781 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "Updating alpha to: 0.0\n",
      "epoch:400 loss:-0.61713 nll:-0.6171 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:4|619 num_lr_updates:0\n",
      "Updating alpha to: 0.0\n",
      "epoch:500 loss:0.65237 nll:0.6524 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:5|931 num_lr_updates:1\n",
      "epoch:600 loss:0.24011 nll:0.24013 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:5|2531 num_lr_updates:1\n",
      "epoch:700 loss:0.37293 nll:0.37296 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:5|4131 num_lr_updates:1\n",
      "epoch:800 loss:0.25439 nll:0.25442 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:5|5731 num_lr_updates:1\n",
      "epoch:900 loss:-0.00059868 nll:-0.00040537 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:5|7331 num_lr_updates:1\n",
      "{'adjacency_f1': 0.0, 'orientation_f1': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~ LINGAUSS ~~~~~~~~\n",
    "csuite_dataset = lingauss\n",
    "\n",
    "# Without discretization\n",
    "print(\"~~~~~ WITHOUT DISCRETIZATION ~~~~~\")\n",
    "df, gt_graph = csuite_dataset(2000, True, True)\n",
    "pred_graph = causica_deci(df, train_config)\n",
    "print(eval_all(torch.tensor(pred_graph), gt_graph))\n",
    "\n",
    "# With discretization\n",
    "print(\"\\n\\n\\n~~~~~ WITH DISCRETIZATION ~~~~~\")\n",
    "df = hartemink_discretization(df)\n",
    "pred_graph = causica_deci(df, train_config)\n",
    "print(eval_all(torch.tensor(pred_graph), gt_graph))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T23:24:29.009256500Z",
     "start_time": "2024-05-11T23:16:44.930080300Z"
    }
   },
   "id": "e52bada10f149ba3"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~ WITHOUT DISCRETIZATION ~~~~~\n",
      "epoch:0 loss:2.9267 nll:2.9247 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:0|1 num_lr_updates:0\n",
      "epoch:100 loss:2.1235 nll:2.121 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:0|1601 num_lr_updates:0\n",
      "Updating alpha to: 0.0\n",
      "epoch:200 loss:2.0187 nll:2.0162 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:1|531 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "epoch:300 loss:1.9144 nll:1.9119 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:2|691 num_lr_updates:0\n",
      "Updating alpha to: 0.0\n",
      "epoch:400 loss:2.0911 nll:2.0886 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:3|550 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "Updating alpha to: 0.0\n",
      "epoch:500 loss:1.9914 nll:1.9889 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|23 num_lr_updates:0\n",
      "epoch:600 loss:1.7264 nll:1.7239 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|1623 num_lr_updates:3\n",
      "epoch:700 loss:1.9537 nll:1.9512 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|3223 num_lr_updates:3\n",
      "epoch:800 loss:1.9224 nll:1.9199 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|4823 num_lr_updates:3\n",
      "epoch:900 loss:1.8718 nll:1.8693 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|6423 num_lr_updates:3\n",
      "{'adjacency_f1': 1.0, 'orientation_f1': 1.0}\n",
      "\n",
      "\n",
      "\n",
      "~~~~~ WITH DISCRETIZATION ~~~~~\n",
      "Working on the 60th level of discretization.\n",
      "Working on the 50th level of discretization.\n",
      "Working on the 40th level of discretization.\n",
      "Working on the 30th level of discretization.\n",
      "Working on the 20th level of discretization.\n",
      "Working on the 10th level of discretization.\n",
      "epoch:0 loss:3.6332 nll:3.6337 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:0|1 num_lr_updates:0\n",
      "epoch:100 loss:-2.9876 nll:-2.9898 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:0|1601 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "epoch:200 loss:-0.21444 nll:-0.21375 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:1|256 num_lr_updates:0\n",
      "Updating alpha to: 0.0\n",
      "epoch:300 loss:-1.6127 nll:-1.6151 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:2|964 num_lr_updates:1\n",
      "epoch:400 loss:-2.4653 nll:-2.4678 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:2|2564 num_lr_updates:2\n",
      "Updating alpha to: 0.0\n",
      "Updating alpha to: 0.0\n",
      "epoch:500 loss:-1.7842 nll:-1.7867 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:4|512 num_lr_updates:0\n",
      "Updating alpha to: 0.0\n",
      "epoch:600 loss:-1.0012 nll:-1.0037 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|283 num_lr_updates:0\n",
      "epoch:700 loss:-4.5156 nll:-4.5181 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|1883 num_lr_updates:3\n",
      "epoch:800 loss:-4.5727 nll:-4.5752 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|3483 num_lr_updates:3\n",
      "epoch:900 loss:-4.6244 nll:-4.6269 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|5083 num_lr_updates:3\n",
      "{'adjacency_f1': 1.0, 'orientation_f1': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~ LINEXP ~~~~~~~~\n",
    "csuite_dataset = linexp\n",
    "\n",
    "# Without discretization\n",
    "print(\"~~~~~ WITHOUT DISCRETIZATION ~~~~~\")\n",
    "df, gt_graph = csuite_dataset(2000, True, True)\n",
    "pred_graph = causica_deci(df, train_config)\n",
    "print(eval_all(torch.tensor(pred_graph), gt_graph))\n",
    "\n",
    "# With discretization\n",
    "print(\"\\n\\n\\n~~~~~ WITH DISCRETIZATION ~~~~~\")\n",
    "df = hartemink_discretization(df)\n",
    "pred_graph = causica_deci(df, train_config)\n",
    "print(eval_all(torch.tensor(pred_graph), gt_graph))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T23:32:58.789195900Z",
     "start_time": "2024-05-11T23:24:29.009256500Z"
    }
   },
   "id": "8b643ea88af5c448"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~ WITHOUT DISCRETIZATION ~~~~~\n",
      "epoch:0 loss:3.6213 nll:3.6193 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:0|1 num_lr_updates:0\n",
      "epoch:100 loss:2.2258 nll:2.2233 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:0|1601 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "epoch:200 loss:2.3897 nll:2.3873 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:1|1221 num_lr_updates:2\n",
      "Updating alpha to: 0.0\n",
      "Updating alpha to: 0.0\n",
      "epoch:300 loss:2.1486 nll:2.1461 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:3|703 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "epoch:400 loss:2.3465 nll:2.344 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:4|1155 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "epoch:500 loss:2.1007 nll:2.0982 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|1398 num_lr_updates:2\n",
      "epoch:600 loss:2.0261 nll:2.0236 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|2998 num_lr_updates:3\n",
      "epoch:700 loss:2.1283 nll:2.1258 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|4598 num_lr_updates:3\n",
      "epoch:800 loss:2.3355 nll:2.333 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|6198 num_lr_updates:3\n",
      "epoch:900 loss:2.458 nll:2.4555 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|7798 num_lr_updates:3\n",
      "{'adjacency_f1': 1.0, 'orientation_f1': 1.0}\n",
      "\n",
      "\n",
      "\n",
      "~~~~~ WITH DISCRETIZATION ~~~~~\n",
      "Working on the 60th level of discretization.\n",
      "Working on the 50th level of discretization.\n",
      "Working on the 40th level of discretization.\n",
      "Working on the 30th level of discretization.\n",
      "Working on the 20th level of discretization.\n",
      "Working on the 10th level of discretization.\n",
      "epoch:0 loss:2.9398 nll:2.9353 dagness:1.08616 num_edges:2 alpha:0 rho:1 step:0|1 num_lr_updates:0\n",
      "Updating alpha to: 0.0\n",
      "epoch:100 loss:0.34631 nll:0.34389 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:1|174 num_lr_updates:0\n",
      "epoch:200 loss:-2.1531 nll:-2.1555 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:1|1774 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "epoch:300 loss:-2.4069 nll:-2.4093 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:2|374 num_lr_updates:0\n",
      "epoch:400 loss:-3.5946 nll:-3.597 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:2|1974 num_lr_updates:2\n",
      "Updating alpha to: 0.0\n",
      "Updating alpha to: 0.0\n",
      "epoch:500 loss:0.0036555 nll:0.0040083 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:4|500 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "epoch:600 loss:-2.0555 nll:-2.0576 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|1523 num_lr_updates:1\n",
      "epoch:700 loss:-3.4324 nll:-3.4321 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:5|3123 num_lr_updates:2\n",
      "epoch:800 loss:-3.7244 nll:-3.7241 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:5|4723 num_lr_updates:2\n",
      "epoch:900 loss:-3.8423 nll:-3.8445 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:5|6323 num_lr_updates:2\n",
      "{'adjacency_f1': 1.0, 'orientation_f1': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~ NONLINGAUSS ~~~~~~~~\n",
    "csuite_dataset = nonlingauss\n",
    "\n",
    "# Without discretization\n",
    "print(\"~~~~~ WITHOUT DISCRETIZATION ~~~~~\")\n",
    "df, gt_graph = csuite_dataset(2000, True, True)\n",
    "pred_graph = causica_deci(df, train_config)\n",
    "print(eval_all(torch.tensor(pred_graph), gt_graph))\n",
    "\n",
    "# With discretization\n",
    "print(\"\\n\\n\\n~~~~~ WITH DISCRETIZATION ~~~~~\")\n",
    "df = hartemink_discretization(df)\n",
    "pred_graph = causica_deci(df, train_config)\n",
    "print(eval_all(torch.tensor(pred_graph), gt_graph))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T23:41:53.193696600Z",
     "start_time": "2024-05-11T23:32:58.789195900Z"
    }
   },
   "id": "8c41586ef6eaf18d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~ WITHOUT DISCRETIZATION ~~~~~\n",
      "epoch:0 loss:6.588 nll:6.5914 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:0|1 num_lr_updates:0\n",
      "epoch:100 loss:1.1577 nll:1.1428 dagness:1.18629 num_edges:6 alpha:0 rho:1 step:0|1601 num_lr_updates:1\n",
      "Updating alpha to: 1.1862878799438477\n",
      "epoch:200 loss:0.96578 nll:0.95008 dagness:1.18629 num_edges:6 alpha:1.1863 rho:1 step:1|1205 num_lr_updates:1\n",
      "Updating rho, dag penalty prev:  1.1862878799\n",
      "epoch:300 loss:0.82451 nll:0.80882 dagness:1.18629 num_edges:6 alpha:1.1863 rho:10 step:2|587 num_lr_updates:0\n",
      "Updating rho, dag penalty prev:  1.1862878799\n",
      "epoch:400 loss:0.78335 nll:0.76763 dagness:1.18629 num_edges:6 alpha:1.1863 rho:100 step:3|69 num_lr_updates:0\n",
      "Updating rho, dag penalty prev:  1.1862878799\n",
      "epoch:500 loss:0.83881 nll:0.82294 dagness:1.18629 num_edges:6 alpha:1.1863 rho:1000 step:4|406 num_lr_updates:0\n",
      "Updating rho, dag penalty prev:  1.1862878799\n",
      "epoch:600 loss:0.98426 nll:0.9668 dagness:1.18629 num_edges:6 alpha:1.1863 rho:10000 step:5|436 num_lr_updates:0\n",
      "Updating rho, dag penalty prev:  1.1862878799\n",
      "epoch:700 loss:0.93132 nll:0.89803 dagness:1.18629 num_edges:6 alpha:1.1863 rho:1e+05 step:6|780 num_lr_updates:0\n",
      "Updating rho, dag penalty prev:  1.1862878799\n",
      "epoch:800 loss:1.1491 nll:0.95754 dagness:1.18629 num_edges:6 alpha:1.1863 rho:1e+06 step:7|80 num_lr_updates:0\n",
      "epoch:900 loss:1.0895 nll:0.8979 dagness:1.18629 num_edges:6 alpha:1.1863 rho:1e+06 step:7|1680 num_lr_updates:2\n",
      "Updating rho, dag penalty prev:  1.1862878799\n",
      "Updating rho, dag penalty prev:  1.1862878799\n",
      "{'adjacency_f1': 0.800000011920929, 'orientation_f1': 0.20000000298023224}\n",
      "\n",
      "\n",
      "\n",
      "~~~~~ WITH DISCRETIZATION ~~~~~\n",
      "Working on the 60th level of discretization.\n",
      "Working on the 50th level of discretization.\n",
      "Working on the 40th level of discretization.\n",
      "Working on the 30th level of discretization.\n",
      "Working on the 20th level of discretization.\n",
      "Working on the 10th level of discretization.\n",
      "epoch:0 loss:9.383 nll:9.3864 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:0|1 num_lr_updates:0\n",
      "epoch:100 loss:-6.2852 nll:-6.2873 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:0|1601 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "epoch:200 loss:-0.84341 nll:-0.84547 dagness:0.00000 num_edges:1 alpha:0 rho:1 step:1|201 num_lr_updates:0\n",
      "Updating alpha to: 0.0\n",
      "epoch:300 loss:-3.2139 nll:-3.2137 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:2|734 num_lr_updates:1\n",
      "epoch:400 loss:-4.7662 nll:-4.766 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:2|2334 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "epoch:500 loss:1.363 nll:1.3635 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:3|934 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "Updating alpha to: 0.0\n",
      "epoch:600 loss:2.9468 nll:2.9474 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:5|364 num_lr_updates:0\n",
      "epoch:700 loss:0.046959 nll:0.047418 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:5|1964 num_lr_updates:1\n",
      "epoch:800 loss:-1.6318 nll:-1.6312 dagness:0.00000 num_edges:0 alpha:0 rho:1 step:5|3564 num_lr_updates:1\n",
      "epoch:900 loss:-2.303 nll:-2.3074 dagness:0.00000 num_edges:2 alpha:0 rho:1 step:5|5164 num_lr_updates:1\n",
      "{'adjacency_f1': 0.0, 'orientation_f1': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~ NONLIN SIMPSON ~~~~~~~~\n",
    "csuite_dataset = nonlin_simpson\n",
    "\n",
    "# Without discretization\n",
    "print(\"~~~~~ WITHOUT DISCRETIZATION ~~~~~\")\n",
    "df, gt_graph = csuite_dataset(2000, True, True)\n",
    "pred_graph = causica_deci(df, train_config)\n",
    "print(eval_all(torch.tensor(pred_graph), gt_graph))\n",
    "\n",
    "# With discretization\n",
    "print(\"\\n\\n\\n~~~~~ WITH DISCRETIZATION ~~~~~\")\n",
    "df = hartemink_discretization(df)\n",
    "pred_graph = causica_deci(df, train_config)\n",
    "print(eval_all(torch.tensor(pred_graph), gt_graph))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T23:58:03.970341900Z",
     "start_time": "2024-05-11T23:41:53.190690800Z"
    }
   },
   "id": "64044b2364d1f697"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~ WITHOUT DISCRETIZATION ~~~~~\n",
      "epoch:0 loss:7.8643 nll:7.8626 dagness:0.00000 num_edges:2 alpha:0 rho:1 step:0|1 num_lr_updates:0\n",
      "epoch:100 loss:1.7906 nll:1.7756 dagness:1.18629 num_edges:6 alpha:0 rho:1 step:0|1601 num_lr_updates:1\n",
      "Updating alpha to: 1.1862878799438477\n",
      "epoch:200 loss:2.235 nll:2.2193 dagness:1.18629 num_edges:6 alpha:1.1863 rho:1 step:1|1206 num_lr_updates:1\n",
      "Updating rho, dag penalty prev:  1.1862878799\n",
      "epoch:300 loss:1.7948 nll:1.7791 dagness:1.18629 num_edges:6 alpha:1.1863 rho:10 step:2|822 num_lr_updates:1\n",
      "Updating rho, dag penalty prev:  1.1862878799\n",
      "epoch:400 loss:1.3995 nll:1.3838 dagness:1.18629 num_edges:6 alpha:1.1863 rho:100 step:3|1011 num_lr_updates:1\n",
      "Updating rho, dag penalty prev:  1.1862878799\n",
      "epoch:500 loss:1.5478 nll:1.5319 dagness:1.18629 num_edges:6 alpha:1.1863 rho:1000 step:4|1092 num_lr_updates:1\n",
      "Updating rho, dag penalty prev:  1.1862878799\n",
      "epoch:600 loss:1.9512 nll:1.9338 dagness:1.18629 num_edges:6 alpha:1.1863 rho:10000 step:5|651 num_lr_updates:0\n",
      "Updating rho, dag penalty prev:  1.1862878799\n",
      "epoch:700 loss:1.834 nll:1.8007 dagness:1.18629 num_edges:6 alpha:1.1863 rho:1e+05 step:6|346 num_lr_updates:0\n",
      "Updating rho, dag penalty prev:  1.1862878799\n",
      "epoch:800 loss:1.825 nll:1.6333 dagness:1.18629 num_edges:6 alpha:1.1863 rho:1e+06 step:7|135 num_lr_updates:0\n",
      "epoch:900 loss:1.8702 nll:1.6785 dagness:1.18629 num_edges:6 alpha:1.1863 rho:1e+06 step:7|1735 num_lr_updates:2\n",
      "Updating rho, dag penalty prev:  1.1862878799\n",
      "Updating rho, dag penalty prev:  1.1862878799\n",
      "{'adjacency_f1': 0.800000011920929, 'orientation_f1': 0.20000000298023224}\n",
      "\n",
      "\n",
      "\n",
      "~~~~~ WITH DISCRETIZATION ~~~~~\n",
      "Working on the 60th level of discretization.\n",
      "Working on the 50th level of discretization.\n",
      "Working on the 40th level of discretization.\n",
      "Working on the 30th level of discretization.\n",
      "Working on the 20th level of discretization.\n",
      "Working on the 10th level of discretization.\n",
      "epoch:0 loss:7.2619 nll:7.2602 dagness:1.08616 num_edges:2 alpha:0 rho:1 step:0|1 num_lr_updates:0\n",
      "Updating alpha to: 0.0\n",
      "epoch:100 loss:-0.66606 nll:-0.67273 dagness:0.00000 num_edges:3 alpha:0 rho:1 step:1|187 num_lr_updates:0\n",
      "epoch:200 loss:-4.6307 nll:-4.6398 dagness:0.00000 num_edges:4 alpha:0 rho:1 step:1|1787 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "epoch:300 loss:-2.3573 nll:-2.3645 dagness:0.00000 num_edges:3 alpha:0 rho:1 step:2|387 num_lr_updates:0\n",
      "epoch:400 loss:-6.3254 nll:-6.3327 dagness:0.00000 num_edges:3 alpha:0 rho:1 step:2|1987 num_lr_updates:2\n",
      "Updating alpha to: 0.0\n",
      "epoch:500 loss:1.3996 nll:1.3898 dagness:0.50417 num_edges:4 alpha:0 rho:1 step:3|587 num_lr_updates:0\n",
      "Updating rho, dag penalty prev:  0.0000000000\n",
      "epoch:600 loss:-6.6236 nll:-6.6336 dagness:0.50417 num_edges:4 alpha:0 rho:10 step:4|1334 num_lr_updates:2\n",
      "Updating alpha to: 0.0\n",
      "epoch:700 loss:-3.0912 nll:-3.1012 dagness:0.50417 num_edges:4 alpha:0 rho:10 step:5|1131 num_lr_updates:1\n",
      "Updating rho, dag penalty prev:  0.0000000000\n",
      "Updating rho, dag penalty prev:  0.0000000000\n",
      "epoch:800 loss:-4.5142 nll:-4.5242 dagness:0.50417 num_edges:4 alpha:0 rho:1000 step:7|629 num_lr_updates:1\n",
      "Updating rho, dag penalty prev:  0.0000000000\n",
      "epoch:900 loss:-1.6616 nll:-1.6719 dagness:0.50417 num_edges:4 alpha:0 rho:10000 step:8|339 num_lr_updates:0\n",
      "Updating rho, dag penalty prev:  0.0000000000\n",
      "{'adjacency_f1': 0.75, 'orientation_f1': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~ SYMPROD SIMPSON ~~~~~~~~\n",
    "csuite_dataset = symprod_simpson\n",
    "\n",
    "# Without discretization\n",
    "print(\"~~~~~ WITHOUT DISCRETIZATION ~~~~~\")\n",
    "df, gt_graph = csuite_dataset(2000, True, True)\n",
    "pred_graph = causica_deci(df, train_config)\n",
    "print(eval_all(torch.tensor(pred_graph), gt_graph))\n",
    "\n",
    "# With discretization\n",
    "print(\"\\n\\n\\n~~~~~ WITH DISCRETIZATION ~~~~~\")\n",
    "df = hartemink_discretization(df)\n",
    "pred_graph = causica_deci(df, train_config)\n",
    "print(eval_all(torch.tensor(pred_graph), gt_graph))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T00:14:34.903585100Z",
     "start_time": "2024-05-11T23:58:03.969342900Z"
    }
   },
   "id": "6d2fd8eefa253ad6"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~ WITHOUT DISCRETIZATION ~~~~~\n",
      "epoch:0 loss:15.098 nll:15.069 dagness:6.18400 num_edges:20 alpha:0 rho:1 step:0|1 num_lr_updates:0\n",
      "epoch:100 loss:6.4576 nll:6.4092 dagness:6.45935 num_edges:21 alpha:0 rho:1 step:0|1601 num_lr_updates:1\n",
      "Updating alpha to: 7.314506530761719\n",
      "epoch:200 loss:6.6152 nll:6.5513 dagness:4.73215 num_edges:19 alpha:7.3145 rho:1 step:1|646 num_lr_updates:0\n",
      "Updating rho, dag penalty prev:  7.3145065308\n",
      "epoch:300 loss:6.2776 nll:6.2102 dagness:4.92532 num_edges:20 alpha:7.3145 rho:10 step:2|404 num_lr_updates:0\n",
      "epoch:400 loss:6.2949 nll:6.2303 dagness:4.73215 num_edges:19 alpha:7.3145 rho:10 step:2|2004 num_lr_updates:1\n",
      "Updating alpha to: 54.63604736328125\n",
      "epoch:500 loss:6.6028 nll:6.4261 dagness:4.73215 num_edges:19 alpha:54.636 rho:10 step:3|1414 num_lr_updates:2\n",
      "Updating rho, dag penalty prev:  4.7321538925\n",
      "epoch:600 loss:6.436 nll:6.2765 dagness:4.18519 num_edges:18 alpha:54.636 rho:100 step:4|1131 num_lr_updates:1\n",
      "Updating rho, dag penalty prev:  4.7321538925\n",
      "epoch:700 loss:6.833 nll:6.6716 dagness:4.18519 num_edges:18 alpha:54.636 rho:1000 step:5|495 num_lr_updates:0\n",
      "Updating rho, dag penalty prev:  4.7321538925\n",
      "epoch:800 loss:6.8873 nll:6.7062 dagness:4.18519 num_edges:18 alpha:54.636 rho:10000 step:6|272 num_lr_updates:0\n",
      "epoch:900 loss:7.0069 nll:6.8257 dagness:4.18519 num_edges:18 alpha:54.636 rho:10000 step:6|1872 num_lr_updates:2\n",
      "Updating rho, dag penalty prev:  4.7321538925\n",
      "{'adjacency_f1': 0.75, 'orientation_f1': 0.3333333432674408}\n",
      "\n",
      "\n",
      "\n",
      "~~~~~ WITH DISCRETIZATION ~~~~~\n",
      "Working on the 60th level of discretization.\n",
      "Working on the 50th level of discretization.\n",
      "Working on the 40th level of discretization.\n",
      "Working on the 30th level of discretization.\n",
      "Working on the 20th level of discretization.\n",
      "Working on the 10th level of discretization.\n",
      "epoch:0 loss:15.28 nll:15.248 dagness:6.89144 num_edges:21 alpha:0 rho:1 step:0|1 num_lr_updates:0\n",
      "Updating alpha to: 0.0\n",
      "epoch:100 loss:-6.9376 nll:-6.9604 dagness:0.00000 num_edges:11 alpha:0 rho:1 step:1|110 num_lr_updates:0\n",
      "Updating alpha to: 0.0\n",
      "Updating rho, dag penalty prev:  0.0000000000\n",
      "epoch:200 loss:4.98 nll:4.9576 dagness:0.33373 num_edges:10 alpha:0 rho:10 step:3|23 num_lr_updates:0\n",
      "epoch:300 loss:-16.024 nll:-16.042 dagness:0.16677 num_edges:8 alpha:0 rho:10 step:3|1623 num_lr_updates:1\n",
      "Updating rho, dag penalty prev:  0.0000000000\n",
      "epoch:400 loss:-4.0968 nll:-4.1126 dagness:0.16677 num_edges:7 alpha:0 rho:100 step:4|223 num_lr_updates:0\n",
      "Updating rho, dag penalty prev:  0.0000000000\n",
      "Updating rho, dag penalty prev:  0.0000000000\n",
      "epoch:500 loss:-3.0181 nll:-3.039 dagness:0.16677 num_edges:9 alpha:0 rho:10000 step:6|164 num_lr_updates:0\n",
      "epoch:600 loss:-9.3897 nll:-9.4076 dagness:0.00000 num_edges:8 alpha:0 rho:10000 step:6|1764 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "epoch:700 loss:-6.0053 nll:-6.0364 dagness:0.00000 num_edges:13 alpha:0 rho:10000 step:7|783 num_lr_updates:0\n",
      "epoch:800 loss:-14.619 nll:-14.653 dagness:0.00000 num_edges:14 alpha:0 rho:10000 step:7|2383 num_lr_updates:2\n",
      "Updating alpha to: 0.0\n",
      "Updating rho, dag penalty prev:  0.0000000000\n",
      "epoch:900 loss:-3.6692 nll:-3.7049 dagness:0.00000 num_edges:15 alpha:0 rho:1e+05 step:9|690 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "{'adjacency_f1': 0.5185185074806213, 'orientation_f1': 0.222222238779068}\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~ LARGE BACKDOOR ~~~~~~~~\n",
    "csuite_dataset = large_backdoor\n",
    "\n",
    "# Without discretization\n",
    "print(\"~~~~~ WITHOUT DISCRETIZATION ~~~~~\")\n",
    "df, gt_graph = csuite_dataset(2000, True, True)\n",
    "pred_graph = causica_deci(df, train_config)\n",
    "print(eval_all(torch.tensor(pred_graph), gt_graph))\n",
    "\n",
    "# With discretization\n",
    "print(\"\\n\\n\\n~~~~~ WITH DISCRETIZATION ~~~~~\")\n",
    "df = hartemink_discretization(df)\n",
    "pred_graph = causica_deci(df, train_config)\n",
    "print(eval_all(torch.tensor(pred_graph), gt_graph))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T00:56:13.344690700Z",
     "start_time": "2024-05-12T00:14:34.903265900Z"
    }
   },
   "id": "40ae5cdf1117da57"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~ WITHOUT DISCRETIZATION ~~~~~\n",
      "epoch:0 loss:15.5 nll:15.468 dagness:5.24694 num_edges:21 alpha:0 rho:1 step:0|1 num_lr_updates:0\n",
      "epoch:100 loss:7.9156 nll:7.8507 dagness:12.01619 num_edges:27 alpha:0 rho:1 step:0|1601 num_lr_updates:1\n",
      "Updating alpha to: 11.288244247436523\n",
      "epoch:200 loss:7.6172 nll:7.5251 dagness:6.63588 num_edges:22 alpha:11.288 rho:1 step:1|880 num_lr_updates:0\n",
      "Updating alpha to: 17.92412567138672\n",
      "epoch:300 loss:7.4576 nll:7.3433 dagness:6.63588 num_edges:22 alpha:17.924 rho:1 step:2|592 num_lr_updates:0\n",
      "Updating rho, dag penalty prev:  6.6358814240\n",
      "epoch:400 loss:7.1753 nll:7.0609 dagness:6.63588 num_edges:22 alpha:17.924 rho:10 step:3|714 num_lr_updates:1\n",
      "Updating rho, dag penalty prev:  6.6358814240\n",
      "epoch:500 loss:7.0205 nll:6.9056 dagness:6.63588 num_edges:22 alpha:17.924 rho:100 step:4|652 num_lr_updates:1\n",
      "Updating rho, dag penalty prev:  6.6358814240\n",
      "epoch:600 loss:6.8398 nll:6.7199 dagness:6.63588 num_edges:22 alpha:17.924 rho:1000 step:5|963 num_lr_updates:1\n",
      "Updating rho, dag penalty prev:  6.6358814240\n",
      "epoch:700 loss:7.3047 nll:7.1353 dagness:6.63588 num_edges:22 alpha:17.924 rho:10000 step:6|826 num_lr_updates:1\n",
      "Updating rho, dag penalty prev:  6.6358814240\n",
      "epoch:800 loss:7.477 nll:6.9717 dagness:5.67292 num_edges:21 alpha:17.924 rho:1e+05 step:7|962 num_lr_updates:1\n",
      "Updating rho, dag penalty prev:  6.6358814240\n",
      "epoch:900 loss:10.295 nll:7.1731 dagness:4.92195 num_edges:20 alpha:17.924 rho:1e+06 step:8|109 num_lr_updates:0\n",
      "{'adjacency_f1': 0.5161290764808655, 'orientation_f1': 0.25806453824043274}\n",
      "\n",
      "\n",
      "\n",
      "~~~~~ WITH DISCRETIZATION ~~~~~\n",
      "Working on the 60th level of discretization.\n",
      "Working on the 50th level of discretization.\n",
      "Working on the 40th level of discretization.\n",
      "Working on the 30th level of discretization.\n",
      "Working on the 20th level of discretization.\n",
      "Working on the 10th level of discretization.\n",
      "epoch:0 loss:15.709 nll:15.699 dagness:1.08616 num_edges:12 alpha:0 rho:1 step:0|1 num_lr_updates:0\n",
      "epoch:100 loss:-10.343 nll:-10.362 dagness:0.00000 num_edges:11 alpha:0 rho:1 step:0|1601 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "epoch:200 loss:1.4047 nll:1.3856 dagness:0.50418 num_edges:10 alpha:0 rho:1 step:1|201 num_lr_updates:0\n",
      "Updating alpha to: 0.0\n",
      "Updating alpha to: 0.0\n",
      "epoch:300 loss:3.6779 nll:3.6639 dagness:0.00000 num_edges:7 alpha:0 rho:1 step:3|11 num_lr_updates:0\n",
      "Updating rho, dag penalty prev:  0.0000000000\n",
      "epoch:400 loss:-8.149 nll:-8.1745 dagness:0.50418 num_edges:11 alpha:0 rho:10 step:4|375 num_lr_updates:0\n",
      "Updating rho, dag penalty prev:  0.0000000000\n",
      "epoch:500 loss:-4.788 nll:-4.8209 dagness:2.32455 num_edges:14 alpha:0 rho:100 step:5|1077 num_lr_updates:1\n",
      "Updating rho, dag penalty prev:  0.0000000000\n",
      "epoch:600 loss:7.8393 nll:7.806 dagness:1.39816 num_edges:14 alpha:0 rho:1000 step:6|22 num_lr_updates:0\n",
      "epoch:700 loss:-13.099 nll:-13.131 dagness:1.17651 num_edges:13 alpha:0 rho:1000 step:6|1622 num_lr_updates:2\n",
      "Updating rho, dag penalty prev:  0.0000000000\n",
      "Updating rho, dag penalty prev:  0.0000000000\n",
      "epoch:800 loss:0.56202 nll:0.52579 dagness:0.16677 num_edges:15 alpha:0 rho:1e+05 step:8|748 num_lr_updates:1\n",
      "Updating alpha to: 0.0\n",
      "Updating alpha to: 0.0\n",
      "epoch:900 loss:-9.7469 nll:-9.781 dagness:0.00000 num_edges:14 alpha:0 rho:1e+05 step:10|864 num_lr_updates:1\n",
      "{'adjacency_f1': 0.3448276221752167, 'orientation_f1': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~~ WEAK ARROWS ~~~~~~~~\n",
    "csuite_dataset = weak_arrows\n",
    "\n",
    "# Without discretization\n",
    "print(\"~~~~~ WITHOUT DISCRETIZATION ~~~~~\")\n",
    "df, gt_graph = csuite_dataset(2000, True, True)\n",
    "pred_graph = causica_deci(df, train_config)\n",
    "print(eval_all(torch.tensor(pred_graph), gt_graph))\n",
    "\n",
    "# With discretization\n",
    "print(\"\\n\\n\\n~~~~~ WITH DISCRETIZATION ~~~~~\")\n",
    "df = hartemink_discretization(df)\n",
    "pred_graph = causica_deci(df, train_config)\n",
    "print(eval_all(torch.tensor(pred_graph), gt_graph))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T01:35:00.470648400Z",
     "start_time": "2024-05-12T00:56:13.339847300Z"
    }
   },
   "id": "33d80c858a8123d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Conclusions\n",
    "\n",
    "Very good adjacency scores all around, and good results for the simpler graphs for the orientation too. However, the model starts to struggle on the orientation prediction as the complexity of the graph that needs to be predicted increases.\n",
    "\n",
    "As expected, almost overall significantly worse results for the discretized data, due to the loss in information, and to the fact that the network used is not designed for categorical data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89aeb3eb86a83c81"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
